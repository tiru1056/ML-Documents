{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FALL 2020 -> CSE 6363 -> Machine Learning\n",
    "## Assignment 2\n",
    "### Problem 2\n",
    "### Name:  Tirumala Manukonda (UTA ID# 1001662386)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation multi-class logistic regression using a soft-max function and cross entropy\n",
    "__Summary__:\n",
    "\n",
    "- Read in Train and Test data set\n",
    "- Using only images within class 0 to 5(for Train and Test Data)\n",
    "- Implemented Multi-class Logistic regression using softmax and Cross entropy loss function\n",
    "  - Softmax Regression is a form of logistic regression that normalizes an input value into \n",
    "    a vector of values that follows a probability distribution whose total sums up to 1.\n",
    "  - The output values are between the range [0,1]\n",
    "  - we are able to avoid binary classification and accommodate as many classes or dimensions \n",
    "    in our model\n",
    "  -  softmax is sometimes referred to as a multinomial logistic regression.\n",
    "- Classification report has given the values of our multi class logistic regression model\n",
    "- Accuracy,Precision and Recall score are used for analysing the performance of our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
      "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "2        4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "3        1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "5        2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "6        1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "59985    2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "59987    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "59991    2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "59994    1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "59996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "\n",
      "       778  779  780  781  782  783  784  \n",
      "1        0    0    0    0    0    0    0  \n",
      "2        0    0    0    0    0    0    0  \n",
      "3        0    0    0    0    0    0    0  \n",
      "5        0    0    0    0    0    0    0  \n",
      "6        0    0    0    0    0    0    0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  \n",
      "59985    0    0    0    0    0    0    0  \n",
      "59987    0    0    0    0    0    0    0  \n",
      "59991    0    0    0    0    0    0    0  \n",
      "59994    0    0    0    0    0    0    0  \n",
      "59996    0    0    0    0    0    0    0  \n",
      "\n",
      "[30596 rows x 785 columns]\n",
      "      0    1    2    3    4    5    6    7    8    9    ...  775  776  777  \\\n",
      "1       2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "2       1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "3       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "4       4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "5       1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "9993    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9994    1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9995    2    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9996    3    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9997    4    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "\n",
      "      778  779  780  781  782  783  784  \n",
      "1       0    0    0    0    0    0    0  \n",
      "2       0    0    0    0    0    0    0  \n",
      "3       0    0    0    0    0    0    0  \n",
      "4       0    0    0    0    0    0    0  \n",
      "5       0    0    0    0    0    0    0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "9993    0    0    0    0    0    0    0  \n",
      "9994    0    0    0    0    0    0    0  \n",
      "9995    0    0    0    0    0    0    0  \n",
      "9996    0    0    0    0    0    0    0  \n",
      "9997    0    0    0    0    0    0    0  \n",
      "\n",
      "[5139 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reading the csv file from the local folder and displaying top 10 values\n",
    "\n",
    "Train_df = pd.read_csv(\"C:/Users/Dell/OneDrive/Documents/mnist_train.csv\",sep=',',header=None) \n",
    "indexNames = Train_df[ Train_df[0] > 4].index\n",
    "Train_df.drop(indexNames , inplace=True)\n",
    "print(Train_df)\n",
    "\n",
    "\n",
    "Test_df = pd.read_csv(\"C:/Users/Dell/OneDrive/Documents/mnist_test.csv\",sep=',',header=None) \n",
    "indexNames = Test_df[ Test_df[0] > 4].index\n",
    "Test_df.drop(indexNames , inplace=True)\n",
    "print(Test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assiging the class labels to y train and test data\n",
    "X_train = Train_df.iloc[:,1:]\n",
    "y_train =  Train_df[0]\n",
    "\n",
    "X_test = Test_df.iloc[:,1:]\n",
    "y_test =  Test_df[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Function\n",
    "#Compute the softmax of vector e\n",
    "\n",
    "def softmax(e):\n",
    "    e -= np.max(e)\n",
    "    sm = (np.exp(e).T / np.sum(np.exp(e),axis=1)).T\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic  \n",
    "  function to multiple dimensions\n",
    "- In the softmax function we compute the exponential of the input parameter and the sum of exponential parameters of all \n",
    "  existing values in the inputs. Our output for the Softmax function is the ratio of the exponential of the parameter \n",
    "  and the   sum of  exponential parameter.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts integer class coding into one-hot varient\n",
    "\n",
    "def oneHotEncoder(Y):\n",
    "    m = Y.shape[0]\n",
    "    #Y = Y[:,0]\n",
    "    OHE = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHE = np.array(OHE.todense()).T\n",
    "    return OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- oneHotEncoder function converts integer class coding, where there is a unidimensional array of labels into a one-hot \n",
    "  varient, where array is size of m * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine the probabilities and predictions\n",
    "def getProbsAndPreds(someX):\n",
    "    probs = softmax(np.dot(someX,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To determine the probabilities and predictions of each class for given input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function To compute loss and gradient\n",
    "def LossFunction(w,x,y,lam):\n",
    "    m = x.shape[0] \n",
    "    y_mat = oneHotEncoder(y) \n",
    "    scores = np.dot(x,w) \n",
    "    prob = softmax(scores) \n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) \n",
    "    grad = (-1 / m) * np.dot(x.T,(y_mat - prob)) + lam*w \n",
    "    return loss,grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss function gets training examples, then one hot encoding of the data, computes the scores of input data\n",
    "  and current weights,perform softmax to get probabilities of scores and then it computes the loss and gradient for the loss\n",
    "- Loss or cost function denoted by y_mat means that only the output of the classifier corresponding to the correct class label\n",
    "  is included in the cost. That is, when computing the cost for an example of the digit “4”, only the output of classifier\n",
    "  4 contributes to the cost.\n",
    "- If the classifier outputs 1 for the training example, then the cost is zero. The cost increases exponentially as \n",
    "  the classifier’s output decreases towards 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get accuracy of the model\n",
    "def getAccuracy(someX,someY):\n",
    "    prob,prede = getProbsAndPreds(someX)\n",
    "    accuracy = sum(prede == someY)/(float(len(someY)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To calculate the accuracies of the model using the prediction of y and probability of the predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9708131781932279\n",
      "Test Accuracy:  0.9729519361743529\n"
     ]
    }
   ],
   "source": [
    "#Calculating loss or cost, with values of lam =1, iterations = 100, learningRate = 0.001\n",
    "\n",
    "w = np.zeros([X_train.shape[1],len(np.unique(y_train))])\n",
    "lam = 1\n",
    "iterations = 10000\n",
    "learningRate = 0.0001\n",
    "losses = []\n",
    "for i in range(0,iterations):\n",
    "    loss,grad = LossFunction(w,X_train,y_train,lam)\n",
    "    losses.append(loss)\n",
    "    w = w - (learningRate * grad)\n",
    "\n",
    "print ('Training Accuracy: ', getAccuracy(X_train,y_train))\n",
    "print ('Test Accuracy: ', getAccuracy(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the max_iterations, Cost fuction calculates the loss and loss gradient on the training dataset\n",
    "  and predicts the test set\n",
    "- Training and testing accuracy are calculated which is 97% and 97%\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.96      0.94      0.95      1032\n",
      "           3       0.95      0.97      0.96      1010\n",
      "           4       0.98      0.97      0.98       982\n",
      "\n",
      "    accuracy                           0.97      5139\n",
      "   macro avg       0.97      0.97      0.97      5139\n",
      "weighted avg       0.97      0.97      0.97      5139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "\n",
    "y_pred_proba,y_pred = getProbsAndPreds(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Analysis_:- \n",
    "- My multiclass classification looking good with the good predictions of individual class labels 0,1,2,3,4 and overall good accuracy of 97 %, precision and recall also 97 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
